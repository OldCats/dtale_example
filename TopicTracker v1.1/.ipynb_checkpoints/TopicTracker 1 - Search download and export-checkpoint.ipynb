{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PubMed Topic Tracker\n",
    "## 1. Search and download\n",
    "\n",
    "This tool allows to build PubMed queries, download entries, parse them and save them to a neat .csv file. It takes as input a PubMed query, and outputs a dataset (i.e: a folder containing a PubMed export, its metadata saved in the log file, and the Medline file for eventually importing the references you are analysing in Zotero or similar software). \n",
    "\n",
    "The output can be explored with the second and third notebooks of this collection.\n",
    "\n",
    "Dependencies:\n",
    "- pandas 1.2.1\n",
    "- IPython 7.19.0\n",
    "- tqdm 4.55.1\n",
    "- shutils 0.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from time import sleep\n",
    "import PubGetParse as pg\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from collections import Counter\n",
    "import re\n",
    "import os\n",
    "from shutil import copy2\n",
    "\n",
    "# Define log file\n",
    "log = \"log.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Log file\n",
    "if os.path.exists(log):\n",
    "    os.remove(log)\n",
    "    open(log, 'w').close()\n",
    "else:\n",
    "    open(log, 'w').close()\n",
    "timestr = time.strftime(\"%Y.%m.%d-%H:%M:%S\")\n",
    "with open(log, \"w\") as f:\n",
    "    f.write(\"# This is a log file. It is saved as .py so that the following notebooks can easily import it and use its information.\\n\\n\")\n",
    "with open(log, \"a\") as f:\n",
    "    f.write(\"# started at: \" + timestr + \"\\n\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First step: definition and segmentation of the query\n",
    "The query must not contain time references. PubMed allows only max. 100k results per query, hence the main query will be splitted in years; e.g: if the time references are 1990 - 1995 the software will run one query for 1990, one for 1991, and so on up to 1995. Detailed information on the segmented queries are saved in the log for reproducibility.\n",
    "\n",
    "Every other PubMed tag can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Definition of the query. The best idea is to define it in PubMed and then copypaste it here.\n",
    "print(\"Important: do not include timepoints in your query, they will be defined via this interface\")\n",
    "pubmed_query = input(\"Paste here your PubMed query:\")\n",
    "year0 = int(input(\"\\nIn order to better manage the amount of results, the query will be segmented by year. \\nFrom what year do you want to start?\"))\n",
    "year1 = int(input(\"Up to what year do you want to search?\"))\n",
    "\n",
    "x = range(year0, (year1 + 1) )\n",
    "yearlist = []\n",
    "for n in x:\n",
    "    yearlist.append(n)\n",
    "querylist = []\n",
    "for x in yearlist:\n",
    "    timequery = \"\\\"\" + str(x) + \"/01/01\" + \"\\\"\" + \"[Date - Publication] : \" + \"\\\"\" + str(x) + \"/12/31\" \"\\\"\" + \"[Date - Publication]\" + \" AND \" + pubmed_query\n",
    "    querylist.append(timequery)\n",
    "\n",
    "displayquery = \"\\\"\" + str(year0) + \"/01/01\" + \"\\\"\" + \"[Date - Publication] : \" + \"\\\"\" + str(year1) + \"/12/31\" \"\\\"\" + \"[Date - Publication]\" + \" AND \" + pubmed_query\n",
    "\n",
    "# Add log\n",
    "timestr = time.strftime(\"%Y.%m.%d-%H:%M:%S\")\n",
    "with open(log, \"a\") as f:\n",
    "    f.write(\"year0 = \" + \"\\\"\" +  str(year0) + \"\\\"\" + \"\\n\")\n",
    "    f.write(\"year1 = \" + \"\\\"\" + str(year1) + \"\\\"\" + \"\\n\")\n",
    "    f.write(\"keywords = \" + \"\\\"\" + pubmed_query + \"\\\"\" + \"\\n\\n\")\n",
    "    f.write(\"'''\\n\")\n",
    "    f.write(\"query = \" + displayquery + \"\\n\") \n",
    "    f.write(\"   Segmented as:\\n\")\n",
    "    for x in querylist:\n",
    "        f.write(x + \"\\n\")\n",
    "    f.write(\"'''\\n\\n\")\n",
    "print(\"\\nThis query will be performed in PubMed, segmented by year:\\n\", displayquery)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Run the queries (segmented by year) and merge all the PubMed IDs in one list\n",
    "pids = []\n",
    "for timequery in tqdm(querylist):\n",
    "    x = pg.get_p_ids(timequery)\n",
    "    pids.extend(x)\n",
    "    sleep(0.3)\n",
    "len1 = len(pids)\n",
    "\n",
    "# Clean duplicates from list\n",
    "pids = list(dict.fromkeys(pids))\n",
    "len2 = len(pids)\n",
    "dropped = len1-len2\n",
    "\n",
    "print(\"Query completed. \" + str(len1) + \" PubMed IDs retrieved. \" + str(dropped) + \" duplicate entries dropped.\\n \")\n",
    "print(\"Downloading the non-duplicate entries, which are \" + str(len2))\n",
    "# Add log\n",
    "timestr = time.strftime(\"%Y.%m.%d-%H:%M:%S\")\n",
    "with open(log, \"a\") as f:\n",
    "    f.write(\"# Query executed at: \" + timestr + \"\\n\")\n",
    "    f.write(\"paper_count_original = \" + \"\\\"\" + str(len1) + \"\\\"\" + \"\\n\")\n",
    "estimatedtime = round((len2 / 1.5)/60, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving MedLine entries for each one of the IDs and parsing them\n",
    "Here we pass every PubMed ID previously retrieved to the API. The API responds with the MEDLINE record, from which we parse and save what follows:\n",
    "\n",
    "pid, pid_type, year, journal, publisher, title, book_title, abstract, oabstract, authors, editors, language, meshterms, keywords, coi, doi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Here we download every article and we parse it as a list\n",
    "\n",
    "# Add log\n",
    "timestr = time.strftime(\"%Y.%m.%d-%H:%M:%S\")\n",
    "with open(log, \"a\") as f:\n",
    "    f.write(\"# Download started at: \" + timestr + \"\\n\")\n",
    "\n",
    "estimatedtime = round((len(pids) / 1.5)/60, 2)\n",
    "print(\"\\n\\nEstimated time for downloading and parsing: \", estimatedtime, \"minutes. \\nThis assumes 1.5 iterations per second.\\nGo grab yourself a coffee ;)\")\n",
    "\n",
    "# Retrieve and parse entries\n",
    "entrylist = []\n",
    "for pid in tqdm(pids):\n",
    "    x = pg.get_parse_article_re(pid)\n",
    "    entrylist.append(x)\n",
    "    sleep(0.3)#need to slow it down to avoid being kicked from the server, pity.\n",
    "\n",
    "# Add log\n",
    "timestr = time.strftime(\"%Y.%m.%d-%H:%M:%S\")\n",
    "with open(log, \"a\") as f:\n",
    "    f.write(\"# Download finished at: \" + timestr + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## MedLine entries become a neat dataframe\n",
    "Here we check for duplicates using PubMed IDs, we remove articles published outside the time interval specified in the query and create a dataframe with the content of every entry. We finally export it as a .csv file. \n",
    "\n",
    "### Important: some cleaning is performed on the data\n",
    "PubMed saves multiple dates per entry and can include in the results papers published before the desired timepoint because they have been indexed, so added to the database, years later. \n",
    "Hence, to provide clean results, here we remove from the dataset the papers whose actual publication date was outside the scope of the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(entrylist, columns =[\n",
    "    \"p_id\", \"pid_type\", \"year\", \"journal\", \"publisher\", \"title\", \"book_title\", \"abstract\", \"oabstract\", \"authors\", \"editors\", \"language\", \"meshterms\", \"keywords\", \"coi\", \"grant\", \"doi\"])\n",
    "df.index += 1 \n",
    "\n",
    "# Replace empty cells with NA and cast year to INT\n",
    "df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "df[\"year\"] = df[\"year\"].astype('float').astype('Int32')\n",
    "\n",
    "# check time interval\n",
    "lenght0 = len(df.index)\n",
    "df = df.drop(df[df.year < int(year0)].index)\n",
    "df = df.drop(df[df.year > int(year1)].index)\n",
    "df = df.reset_index(drop=True)\n",
    "df.index += 1\n",
    "lenght1 = len(df.index)\n",
    "\n",
    "message = (\"Dropped \" + str(lenght0-lenght1) + \" entries due to publication time outside query parameters.\")\n",
    "print(message)\n",
    "lenght2 = len(df.index)\n",
    "message = (str(lenght2) + \" entries included.\")\n",
    "print(message)\n",
    "\n",
    "# Export the dataframe\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "exportdir = (\"export/\" + timestr)\n",
    "os.mkdir(exportdir)\n",
    "df.to_csv(exportdir + \"/PubMed full records.csv\",  sep=';')\n",
    "\n",
    "print(str(len1) + \" entries found. \" + str(dropped) + \" duplicate entries dropped.\\n \")\n",
    "print(str(len2) + \" records successfully saved to .csv in \" + exportdir + \". \\nYou can go ahead with the analysis :)\")\n",
    "display(df.head(20))\n",
    "\n",
    "# Add log\n",
    "timestr = time.strftime(\"%Y.%m.%d-%H:%M:%S\")\n",
    "with open(log, \"a\") as f:\n",
    "    f.write(\"paper_count_no_duplicates = \" + \"\\\"\" + str(len(df)) + \"\\\"\" + \"\\n\")\n",
    "    f.write(\"# Data exported at: \" + timestr + \" to : \" + exportdir + \"\\n\")\n",
    "    f.write(\"exportdir = \" + \"\\\"\" + exportdir + \"\\\"\" + \"\\n\")\n",
    "\n",
    "# Copy the log file to the export folder as documentation\n",
    "destination_log = exportdir + \"/log.txt\"\n",
    "copy2(log, destination_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating MedLine file\n",
    "\n",
    "Here we create a MedLine file from the entries included in the analysis. The MedLine file can then be used to import the references (and to get the papers) in reference management software, e.g. Zotero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create MedLine file from the dataframe for import in reference management software\n",
    "message = (\"Creating MedLine file from the dataframe...\")\n",
    "print(message)\n",
    "pids_to_get = df[\"p_id\"].tolist()\n",
    "medline_file = \"medline.txt\"\n",
    "medline_new = open(medline_file, \"w\")\n",
    "medline_new.close()\n",
    "\n",
    "for x in tqdm(pids_to_get):\n",
    "    x = str(x)\n",
    "    pg.art_to_medline(x, medline_file)\n",
    "message = (\"MedLine file created.\")\n",
    "print(message)\n",
    "\n",
    "destination_medline = exportdir + \"/medline.txt\"\n",
    "copy2(medline_file, destination_medline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Add log\n",
    "timestr = time.strftime(\"%Y.%m.%d-%H:%M:%S\")\n",
    "with open(log, \"a\") as f:\n",
    "    f.write(\"paper_count_no_duplicates = \" + \"\\\"\" + str(len(df)) + \"\\\"\" + \"\\n\")\n",
    "    f.write(\"# Data exported at: \" + timestr + \" to : \" + exportdir + \"\\n\")\n",
    "    f.write(\"exportdir = \" + \"\\\"\" + exportdir + \"\\\"\" + \"\\n\")\n",
    "\n",
    "# Copy the log file to the export folder as documentation\n",
    "destination_log = exportdir + \"/log.txt\"\n",
    "copy2(log, destination_log)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
