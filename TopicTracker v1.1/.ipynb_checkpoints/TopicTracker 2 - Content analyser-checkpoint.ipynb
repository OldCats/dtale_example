{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PubMed Topic Tracker\n",
    "## 2. Content analyser\n",
    "\n",
    "This tool allows to analyse the trends of entities over time from a .csv file containing pubmed entries. It takes as input a dataset (i.e: a folder containing a PubMed export generated with the first notebook of this collection, its metadata and the Medline file). \n",
    "\n",
    "It outputs a set of .csv files and .svg plots with the trends of keywords, MeSH terms, authors, lemmas in Title/Abstract, amount of COI statements, and lemma trends in COI statements. The .csv files can then be explored further with the third notebook of this collection (interactive data analysis).\n",
    "\n",
    "Dependencies:\n",
    "- pandas 1.2.1\n",
    "- IPython 7.19.0\n",
    "- tqdm 4.55.1\n",
    "- matplotlib 3.3.3\n",
    "- spacy 2.3.5 (+ en_core_web_md)\n",
    "- wordcloud 1.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import collections\n",
    "import pandas as pd\n",
    "import urllib3\n",
    "http = urllib3.PoolManager()\n",
    "from IPython.display import clear_output, Markdown\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from collections import Counter\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import numpy as np\n",
    "import os\n",
    "import spacy\n",
    "import en_core_web_md\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "# Spacy additional configuration\n",
    "stoplist = [\"|\", \"nan\", \"background\", \"introduction\"]\n",
    "nlp = en_core_web_md.load()\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "nlp.max_length = 100000000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Functions\n",
    "\n",
    "# print in markdown\n",
    "def printmd(string, color=None):\n",
    "    colorstr = \"<span style='color:{}'>{}</span>\".format(color, string)\n",
    "    display(Markdown(colorstr))\n",
    "    \n",
    "# create dataframe\n",
    "def create_entity_dataframe(dic2, target_column):\n",
    "    global df_all_status\n",
    "    df_all_status = \"\"\n",
    "    # Count instances\n",
    "    countdic = {}\n",
    "    for key, value in dic2.items():\n",
    "        counter = Counter(dic2[key])\n",
    "        c_df = pd.DataFrame.from_dict(counter, orient='index').reset_index()\n",
    "        c_df = c_df.rename(columns={\"index\": target_column, 0: \"count\"}) # Specify the name of the columns\n",
    "        countdic[key] = c_df\n",
    "    # Check if empty\n",
    "    empty_list = []\n",
    "    for key, value in countdic.items():\n",
    "        empty_list.append(value.empty)\n",
    "    status = all(x == True for x in empty_list)\n",
    "    if status == True:\n",
    "        df_all_status = \"empty\"\n",
    "        return(\"The dataframe is empty. Bummer.\")\n",
    "\n",
    "    else:\n",
    "        # All of this to one dataframe\n",
    "        valuelist = []\n",
    "        keylist = []\n",
    "        for key, value in countdic.items():\n",
    "            valuelist.append(value)\n",
    "            keylist.append(key)\n",
    "\n",
    "        L = []\n",
    "        for x in valuelist:\n",
    "            x = x.set_index(target_column) # Specify the name of the column\n",
    "            L.append(pd.Series(x.values.tolist(), index=x.index))\n",
    "        df_all = pd.concat(L, axis=1, keys=(keylist))\n",
    "        df_all = df_all.fillna('0')\n",
    "        for x in df_all:\n",
    "            df_all[x]= df_all[x].str[0]\n",
    "        df_all = df_all.astype('int')\n",
    "        indexes = [x for x in range(len(yearlist))]\n",
    "\n",
    "        sums = df_all.iloc[:, indexes].sum(axis=1)\n",
    "        df_all = pd.concat([df_all, sums], axis = 1)\n",
    "        df_all = df_all.rename(columns={0: \"total\"})\n",
    "        ### Add descriptive statistics\n",
    "        # Calculate the min\n",
    "        mins = df_all.iloc[:, indexes].min(axis=1) \n",
    "        df_all = pd.concat([df_all, mins], axis = 1)\n",
    "        df_all = df_all.rename(columns={0: \"min\"})\n",
    "\n",
    "        # Calculate the std\n",
    "        stds = df_all.iloc[:, indexes].std(axis=1) \n",
    "        df_all = pd.concat([df_all, stds], axis = 1)\n",
    "        df_all = df_all.rename(columns={0: \"std\"})\n",
    "        df_all[\"std\"] = (df_all[\"std\"].astype('float')).round(2)\n",
    "\n",
    "        # Calculate the mean\n",
    "        means = df_all.iloc[:, indexes].mean(axis=1) \n",
    "        df_all = pd.concat([df_all, means], axis = 1)\n",
    "        df_all = df_all.rename(columns={0: \"mean\"})\n",
    "        df_all[\"mean\"] = (df_all[\"mean\"].astype('float')).round(2)\n",
    "\n",
    "        # Calculate the max\n",
    "        maxs = df_all.iloc[:, indexes].max(axis=1) \n",
    "        df_all = pd.concat([df_all, maxs], axis = 1)\n",
    "        df_all = df_all.rename(columns={0: \"max\"})\n",
    "\n",
    "        # Reorder and reindex\n",
    "        df_all = df_all.sort_values(by=['total'], ascending=False)\n",
    "        df_all = df_all.reset_index()\n",
    "        df_all.index += 1\n",
    "        filename = target_column.capitalize()\n",
    "        df_all.to_csv(exportdir + \"/data/\" + filename + \".csv\",  sep=';') # Specify the name of the file\n",
    "        return df_all\n",
    "\n",
    "# create normalized dataframe\n",
    "def create_normalized_dataframe(df_all, target_column):\n",
    "    df_all_norm = df_all\n",
    "    df_all_norm = df_all_norm.drop(columns = [\"min\", \"std\", \"mean\", \"max\", \"total\"])\n",
    "    for label, content in papercount.items():\n",
    "        df_all_norm[label] = df_all_norm[label]/content\n",
    "    df_all_norm = df_all_norm.fillna(0)\n",
    "\n",
    "    # Calculate the total\n",
    "    indexes = []\n",
    "    for x in range(len(yearlist)):\n",
    "        indexes.append(x)\n",
    "    sums = df_all_norm.iloc[:, indexes].sum(axis=1)\n",
    "    df_all_norm = pd.concat([df_all_norm, sums], axis = 1)\n",
    "    df_all_norm = df_all_norm.rename(columns={0: \"total\"})\n",
    "\n",
    "    # Calculate the min\n",
    "    mins = df_all_norm.iloc[:, indexes].min(axis=1) \n",
    "    df_all_norm = pd.concat([df_all_norm, mins], axis = 1)\n",
    "    df_all_norm = df_all_norm.rename(columns={0: \"min\"})\n",
    "\n",
    "    # Calculate the std\n",
    "    stds = df_all_norm.iloc[:, indexes].std(axis=1) \n",
    "    df_all_norm = pd.concat([df_all_norm, stds], axis = 1)\n",
    "    df_all_norm = df_all_norm.rename(columns={0: \"std\"})\n",
    "    #df_all_norm[\"std\"] = (df_all_norm[\"std\"].astype('float')).round(2)\n",
    "\n",
    "    # Calculate the mean\n",
    "    means = df_all_norm.iloc[:, indexes].mean(axis=1) \n",
    "    df_all_norm = pd.concat([df_all_norm, means], axis = 1)\n",
    "    df_all_norm = df_all_norm.rename(columns={0: \"mean\"})\n",
    "    #df_all_norm[\"mean\"] = (df_all_norm[\"mean\"].astype('float')).round(2)\n",
    "\n",
    "    # Calculate the max\n",
    "    maxs = df_all_norm.iloc[:, indexes].max(axis=1) \n",
    "    df_all_norm = pd.concat([df_all_norm, maxs], axis = 1)\n",
    "    df_all_norm = df_all_norm.rename(columns={0: \"max\"})\n",
    "\n",
    "    # Reorder and reindex\n",
    "    df_all_norm = df_all_norm.sort_values(by=['total'], ascending=False)\n",
    "    df_all_norm = df_all_norm.reset_index()\n",
    "    df_all_norm.index += 1\n",
    "    del df_all_norm['index']\n",
    "    filename = target_column.capitalize()\n",
    "    df_all_norm.to_csv(exportdir + \"/data/\" + filename + \"_norm.csv\",  sep=';') # Specify the name of the file\n",
    "    return df_all_norm\n",
    "\n",
    "# Plot 5 most frequent entities in the dataframe\n",
    "    # organizing and plotting data\n",
    "def plot_df_top(df_all, target_column, wordlist, plot_title):\n",
    "    colnames = list(df_all.columns) \n",
    "    colnames = ([x for x in colnames if x.startswith('df_')])\n",
    "    years = [int(x.replace(\"df_\", \"\")) for x in colnames]\n",
    "    df_all_p = df_all.set_index(target_column)\n",
    "    plotthis = {}\n",
    "    for x in wordlist:\n",
    "        servicelist = []\n",
    "        for y in colnames:\n",
    "            value = df_all_p.at[x,y]\n",
    "            servicelist.append(value)\n",
    "        plotthis[x] = servicelist\n",
    "    for key, value in plotthis.items():\n",
    "        lab = key\n",
    "        x = years\n",
    "        y = value\n",
    "        plt.plot(x, y, label = lab)\n",
    "\n",
    "    # style\n",
    "    plt.style.use('seaborn')  \n",
    "    # naming the x axis \n",
    "    plt.xlabel('Year', fontsize=15) \n",
    "    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    plt.xlim(year0, year1) \n",
    "    # naming the y axis \n",
    "    plt.ylabel('Count', fontsize=15)\n",
    "    plt.yticks(fontsize=12)\n",
    "    # title \n",
    "    plt.title(plot_title, fontsize=20)  \n",
    "    # legend\n",
    "    plt.legend(fontsize=15) \n",
    "    # size\n",
    "    plt.rcParams[\"figure.figsize\"] = (25,8)\n",
    "    # save and show the plot \n",
    "    plt.savefig(exportdir + \"/plots/\" + plot_title + \".svg\")\n",
    "    plt.show()\n",
    "\n",
    "# Plot percentage/count variations\n",
    "# This function expect a df with two rows (count and percent); a title, and either \"count\" or \"percent\"\n",
    "    # organizing and plotting data\n",
    "def plot_variations(df_all, plot_title, entity):\n",
    "    import matplotlib.axes as ax\n",
    "    colnames = list(df_all.columns) \n",
    "    colnames = ([x for x in colnames if x.startswith('df_')])\n",
    "    years = [int(x.replace(\"df_\", \"\")) for x in colnames]\n",
    "    values = []\n",
    "    for y in colnames:\n",
    "        value = df_all.at[entity, y]\n",
    "        values.append(value)\n",
    "    plt.plot(years, values, label = entity)\n",
    "\n",
    "    # style\n",
    "    plt.style.use('seaborn')  \n",
    "    # naming the x axis \n",
    "    plt.xlabel('Year', fontsize=15) \n",
    "    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    # naming the y axis \n",
    "    plt.ylabel(entity.capitalize(), fontsize=15)\n",
    "    plt.yticks(fontsize=12)\n",
    "    # title \n",
    "    plt.title(plot_title + \" - \" + entity, fontsize=20)  \n",
    "    # legend\n",
    "    plt.legend(fontsize=15) \n",
    "    # size\n",
    "    plt.rcParams[\"figure.figsize\"] = (25,8)\n",
    "    # save and show the plot \n",
    "    plt.savefig(exportdir + \"/plots/\" + plot_title + \" - \" + entity + \".svg\")\n",
    "    plt.show()\n",
    "    \n",
    "# Spacy preprocessing functions\n",
    "def is_token_allowed(token):\n",
    "    if (not token or not token.string.strip() or token.is_stop or token.is_punct or token in spacy_stopwords):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def preprocess_token(token):\n",
    "    if is_token_allowed:\n",
    "        return token.lemma_.strip().lower()\n",
    "\n",
    "# Font to use for word clouds\n",
    "font = \"fonts/Roboto-Regular.ttf\"\n",
    "# Function to generate word cloud. expects: dataframe to use, column with entities, column with frequencies, maximum number of words to display, background color\n",
    "def generate_wordcloud(df, column_entities, column_numbers, m_words, bg_color):\n",
    "    lemmalist = df[column_entities].tolist()\n",
    "    freqlist = df[column_numbers].tolist()\n",
    "    zip_iterator = zip(lemmalist, freqlist)\n",
    "    freq_dict = dict(zip_iterator)\n",
    "    wordcloud = WordCloud(width=1000, height=500, background_color = bg_color, font_path = font, min_font_size = 8, max_words=m_words, relative_scaling=1, normalize_plurals=False).generate_from_frequencies(freq_dict)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    # save and show the plot \n",
    "    plt.savefig(exportdir + \"/word clouds/\" + \"word cloud\" + \" - \" + column_entities + \".svg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dataframe\n",
    "Here we create a dataframe importing the .csv of a pre-existing export generated with the first notebook of this collection.\n",
    "\n",
    "If you want to re-run your analysis on existing data, you can restart from here, no need to re-download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create list of datasets\n",
    "dirlist = []\n",
    "dircontent = os.listdir(\"export\")\n",
    "for x in dircontent:\n",
    "    if x.startswith(\".\") == False:\n",
    "        dirlist.append(x)\n",
    "i = 0\n",
    "datalist = collections.defaultdict(list)\n",
    "for x in dirlist:\n",
    "    if \"index\" not in datalist:\n",
    "        datalist[\"index\"] = [i]\n",
    "    else:\n",
    "        datalist[\"index\"].append(i)\n",
    "    i = i+1\n",
    "    if \"dirname\" not in datalist:\n",
    "        datalist[\"dirname\"] = [x]\n",
    "    else:\n",
    "        datalist[\"dirname\"].append(x)\n",
    "\n",
    "    filepath = \"export/\" + x + \"/\" + \"log.txt\"\n",
    "    with open (filepath, \"r\") as f:\n",
    "        file = f.read()\n",
    "    year0 = re.search(r\"(?<=year0\\s=\\s\\\")\\d*\", file)\n",
    "    year0 = year0.group(0)\n",
    "    if \"year0\" not in datalist:\n",
    "        datalist[\"year0\"] = [year0]\n",
    "    else:\n",
    "        datalist[\"year0\"].append(year0)\n",
    "    year1 = re.search(r\"(?<=year1\\s=\\s\\\")\\d*\", file)\n",
    "    year1 = year1.group(0)\n",
    "    if \"year1\" not in datalist:\n",
    "        datalist[\"year1\"] = [year1]\n",
    "    else:\n",
    "        datalist[\"year1\"].append(year1)\n",
    "    keywords = re.search(r\"(?<=keywords\\s=\\s\\\").*(?=\\\")\", file)\n",
    "    keywords = keywords.group(0)\n",
    "    if \"query\" not in datalist:\n",
    "        datalist[\"query\"] = [keywords]\n",
    "    else:\n",
    "        datalist[\"query\"].append(keywords)\n",
    "    paper_count = re.search(r\"(?<=paper_count_no_duplicates\\s=\\s\\\")\\d*\", file)\n",
    "    paper_count = paper_count.group(0)\n",
    "    if \"paper_count\" not in datalist:\n",
    "        datalist[\"paper_count\"] = [paper_count]\n",
    "    else:\n",
    "        datalist[\"paper_count\"].append(paper_count)\n",
    "    querydatetime = re.search(r\"(?<=executed\\sat:\\s).*\", file)\n",
    "    querydatetime = querydatetime.group(0)\n",
    "    if \"querydatetime\" not in datalist:\n",
    "        datalist[\"querydatetime\"] = [querydatetime]\n",
    "    else:\n",
    "        datalist[\"querydatetime\"].append(querydatetime)\n",
    "df_data = pd.DataFrame.from_dict(datalist)\n",
    "df_data = df_data.drop(df_data.columns[0], axis=1)\n",
    "pd.set_option('max_colwidth', 500)\n",
    "df_data.index += 1 \n",
    "printmd(\"\\n\\n## Datasets available: \", color = \"black\")\n",
    "display(df_data)\n",
    "pd.reset_option('max_colwidth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dataset = input(\"specify the dirname of the dataset you want to work with:\")\n",
    "file = \"export/\" + dataset + \"/\" + \"log.txt\"\n",
    "with open (file, \"r\") as f:\n",
    "    file = f.read()\n",
    "year0 = re.search(r\"(?<=year0\\s=\\s\\\")\\d*\", file)\n",
    "year0 = year0.group(0)\n",
    "year1 = re.search(r\"(?<=year1\\s=\\s\\\")\\d*\", file)\n",
    "year1 = year1.group(0)\n",
    "years = []\n",
    "for x in range (int(year0),int(year1)+1):\n",
    "    years.append(x)\n",
    "keywords = re.search(r\"(?<=keywords\\s=\\s\\\").*(?=\\\")\", file)\n",
    "keywords = keywords.group(0)\n",
    "paper_count = re.search(r\"(?<=paper_count_no_duplicates\\s=\\s\\\")\\d*\", file)\n",
    "paper_count = paper_count.group(0)\n",
    "exportdir = re.search(r\"(?<=exportdir\\s=\\s\\\").*(?=\\\")\", file)\n",
    "exportdir = exportdir.group(0)\n",
    "\n",
    "# check for subfolders and in case create them\n",
    "if \"data\" not in os.listdir(exportdir):\n",
    "    os.mkdir(exportdir + \"/data\")\n",
    "    print(\"data folder created\")\n",
    "if \"plots\" not in os.listdir(exportdir):\n",
    "    os.mkdir(exportdir + \"/plots\")\n",
    "    print(\"plots folder created\")\n",
    "if \"word clouds\" not in os.listdir(exportdir):\n",
    "    os.mkdir(exportdir + \"/word clouds\")\n",
    "    print(\"word cloud folder created\")\n",
    "querydatetime = re.search(r\"(?<=executed\\sat:\\s).*\", file)\n",
    "querydatetime = querydatetime.group(0)\n",
    "print(\"\\nUsing dataset: \\\"\" + dataset + \"\\\"\\n\")\n",
    "print(\"Time interval: \" + year0 + \" - \" + year1 + \"\\n\" + \"Query: \" + keywords + \"\\n\" + \"Query date: \" + querydatetime + \"\\n\" + paper_count + \" entries\")\n",
    "\n",
    "# Create the dataframe\n",
    "imported_df = pd.read_csv(exportdir + \"/PubMed full records.csv\", sep=';', low_memory=False) \n",
    "imported_df = imported_df.drop(imported_df.columns[0], axis=1)\n",
    "imported_df.index += 1 \n",
    "\n",
    "printmd(\"Showing the first 20 entries of the dataframe\", color = \"black\")\n",
    "display(imported_df.head(20))\n",
    "\n",
    "# Export the clean dataframe (removing years outside the query)\n",
    "#imported_df.to_csv(exportdir + \"/PubMed full records_years outside query removed.csv\",  sep=';')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing dataframes by year\n",
    "Here we divide the big dataframe in many dataframes, one per year, so that we can confront frequencies over time. \n",
    "\n",
    "### IMPORTANT: queries might yield articles with dates outside the upper limit of the time range (discrepancies between acceptance and publication date in PubMed can happen). that's why I retrieve the years from the data instead than using them from the query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df = imported_df\n",
    "year_list = df['year'].fillna(\"removethis\")\n",
    "year_list = year_list.tolist()\n",
    "year_list = [x for x in year_list if x != \"removethis\"]\n",
    "year0 = int(year0)\n",
    "year1 = int(year1)\n",
    "x = range(int(year0), int(year1) + 1 )\n",
    "df[\"year\"] = df[\"year\"].astype('Int64')\n",
    "yearlist = []\n",
    "querylist = []\n",
    "for n in x:\n",
    "    n = \"df_\" + str(n)\n",
    "    yearlist.append(n)\n",
    "for n in x: \n",
    "    n = \"year == \" + str(n)\n",
    "    querylist.append(n)\n",
    "\n",
    "# create dictionary of dataframes\n",
    "d = {}\n",
    "for x, y in zip(yearlist,querylist):\n",
    "    d[x] = df.query(y) \n",
    "\n",
    "# count papers for normalization\n",
    "papercount = {}\n",
    "for key, value in d.items():\n",
    "    x = len(value.index)\n",
    "    papercount[key] = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on normalization\n",
    "Normalization is performed in the same whay for each entity, i.e:  normalized entity = count of entity / number of papers. So a normalized value of 0.1 for, e.g. a keyword means that said entity is present in 10% of the entries in the subset. This is true for keywords, MeSH terms and authors, but not for anything which is based on lemmas. More details on this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Keyword trends\n",
    "Here we compute the trends for all the keywords in the dataset. The trends are exported to .csv files. The software plots the five most frequent entities in the dataframe. This is basically for reporting; for a more refined analysis use the Data Explorer notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "printmd(\"**Analysis in progress...**\", color = \"red\")\n",
    "\n",
    "# Retrieve objects divided by year and organize them in a dic {key: [object]}\n",
    "list_global = []\n",
    "iterable = d.items()\n",
    "for key, value in tqdm(iterable):\n",
    "    year_list = []\n",
    "    listname = \"list_\" + key\n",
    "    year_list = value['keywords'].tolist() # -> Configure here the column to target\n",
    "    year_list = [x.lower() for x in year_list if str(x) != \"nan\"] # -> All to lowercase to merge duplicates, e.g: \"Ethics\" and \"ethics\"\n",
    "    list_global.append(year_list)\n",
    "dic1 = dict(zip(yearlist, list_global)) # Structure: {df_year: [content, but divided by entry]}\n",
    "\n",
    "# Separate objects in the same string (e.g: authors are separated by commas)\n",
    "typematch = re.compile(r\"(?<=')[\\w\\s]+(?=')\")\n",
    "dic2 = {} # structure: {df_year: [content, but merged]}\n",
    "for key, value in dic1.items():\n",
    "    dic2[key] = []\n",
    "    for x in value:\n",
    "        x = x.replace(\"[\\'\", \"\\'\")\n",
    "        x = x.replace(\"\\']\", \"\\'\")\n",
    "        x = re.findall(typematch, x)\n",
    "        for y in x:\n",
    "            dic2[key].append(y)\n",
    "\n",
    "# Create the dataframe\n",
    "df_keywords = create_entity_dataframe(dic2, \"keywords\")\n",
    "\n",
    "if df_all_status == \"empty\":\n",
    "    print(\"The keywords dataframe is empty. So no normalization and no plots. Big bummer.\")\n",
    "    keyword_file = exportdir + \"/data/Keywords.csv\"\n",
    "    keyword_file_norm = exportdir + \"/data/Keywords_norm.csv\"\n",
    "    with open(keyword_file, \"w\") as empty_csv:\n",
    "        pass\n",
    "    with open(keyword_file_norm, \"w\") as empty_csv:\n",
    "        pass\n",
    "else:\n",
    "    # Create the normalized dataframe\n",
    "    df_keywords_norm = create_normalized_dataframe(df_keywords, \"keywords\")\n",
    "\n",
    "    # Display the dataframe\n",
    "    clear_output()\n",
    "    message = (\"Trends computed. Displaying the trends of the 20 most frequent entities in the dataframe.\")\n",
    "    printmd(\"\\n\\n**\" + message + \"**\\n\\n\", color = \"green\")\n",
    "    printmd(\"**Trends**\", color = \"black\")\n",
    "    display(df_keywords.head(20))\n",
    "\n",
    "    # Generate word cloud\n",
    "    generate_wordcloud(df_keywords, \"keywords\", \"total\", 50, \"white\")\n",
    "\n",
    "    # Get the list of the entities to plot (5 most common entities in the first dataframe)\n",
    "    keywords_wordlist = []\n",
    "    i = 1\n",
    "    while i < 6:\n",
    "        word = df_keywords.at[i, \"keywords\"]\n",
    "        keywords_wordlist.append(word)\n",
    "        i = i+1\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "    printmd(\"**Plots**\", color = \"black\")\n",
    "\n",
    "    # Plot the data\n",
    "    title = \"keywords \" + str(year0) + \" - \" + str(year1)\n",
    "    plot_df_top(df_keywords, \"keywords\", keywords_wordlist, title)\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    # Plot the normalized data\n",
    "    # Important: plotting the same list as above!\n",
    "    title = \"keywords (normalized) \" + str(year0) + \" - \" + str(year1)\n",
    "    plot_df_top(df_keywords_norm, \"keywords\", keywords_wordlist, title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## MeSH terms trends\n",
    "Here we compute the trends for all the MeSH terms in the dataset. The trends are exported to .csv files. The software plots the five most frequent entities in the dataframe. This is basically for reporting; for a more refined analysis use the Data Explorer notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "printmd(\"**Analysis in progress...**\", color = \"red\")\n",
    "\n",
    "# Retrieve objects divided by year and organize them in a dic {key: [object]}\n",
    "list_global = []\n",
    "iterable = d.items()\n",
    "for key, value in iterable:\n",
    "    year_list = []\n",
    "    listname = \"list_\" + key\n",
    "    year_list = value['meshterms'].tolist() # -> Configure here the column to target\n",
    "    year_list = [x.lower() for x in year_list if str(x) != \"nan\"] # -> All to lowercase to merge duplicates, e.g: \"Ethics\" and \"ethics\"\n",
    "    list_global.append(year_list)\n",
    "dic1 = dict(zip(yearlist, list_global)) # Structure: {df_year: [content, but divided by entry]}\n",
    "\n",
    "# Separate objects in the same string (e.g: authors are separated by commas)\n",
    "typematch = re.compile(r\"(?<=')[\\w\\s]+(?=')\")\n",
    "dic2 = {} # structure: {df_year: [content, but merged]}\n",
    "for key, value in dic1.items():\n",
    "    dic2[key] = []\n",
    "    for x in value:\n",
    "        x = x.replace(\"[\\'\", \"\\'\")\n",
    "        x = x.replace(\"\\']\", \"\\'\")\n",
    "        x = re.findall(typematch, x)\n",
    "        for y in x:\n",
    "            dic2[key].append(y)\n",
    "\n",
    "# Create the dataframe\n",
    "df_meshterms = create_entity_dataframe(dic2, \"meshterms\")\n",
    "\n",
    "if df_all_status == \"empty\":\n",
    "    print(\"The meshterms dataframe is empty. So no normalization and no plots. Big bummer.\")\n",
    "    meshterms_file = exportdir + \"/data/Meshterms.csv\"\n",
    "    meshterms_file_norm = exportdir + \"/data/Meshterms_norm.csv\"\n",
    "    with open(meshterms_file, \"w\") as empty_csv:\n",
    "        pass\n",
    "    with open(meshterms_file_norm, \"w\") as empty_csv:\n",
    "        pass\n",
    "else:\n",
    "    # Create the normalized dataframe\n",
    "    df_meshterms_norm = create_normalized_dataframe(df_meshterms, \"meshterms\")\n",
    "\n",
    "    # Display the dataframe\n",
    "    clear_output()\n",
    "    message = (\"Trends computed. Displaying the trends of the 20 most frequent entities in the dataframe.\")\n",
    "    printmd(\"\\n\\n**\" + message + \"**\\n\\n\", color = \"green\")\n",
    "    printmd(\"**Trends**\", color = \"black\")\n",
    "    display(df_meshterms.head(20))\n",
    "\n",
    "    # Generate word cloud\n",
    "    generate_wordcloud(df_meshterms, \"meshterms\", \"total\", 50, \"white\")\n",
    "\n",
    "    # Get the list of the entities to plot (5 most common entities in the first dataframe)\n",
    "    meshterms_wordlist = []\n",
    "    i = 1\n",
    "    while i < 6:\n",
    "        word = df_meshterms.at[i, \"meshterms\"]\n",
    "        meshterms_wordlist.append(word)\n",
    "        i = i+1\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "    printmd(\"**Plots**\", color = \"black\")\n",
    "\n",
    "    # Plot the data\n",
    "    title = \"MeSH terms \" + str(year0) + \" - \" + str(year1)\n",
    "    plot_df_top(df_meshterms, \"meshterms\", meshterms_wordlist, title)\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    # Plot the normalized data\n",
    "    # Important: plotting the same list as above!\n",
    "    title = \"MeSH terms (normalized) \" + str(year0) + \" - \" + str(year1)\n",
    "    plot_df_top(df_meshterms_norm, \"meshterms\", meshterms_wordlist, title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Authors trends\n",
    "Here we compute the trends for all the authors in the dataset. The trends are exported to .csv files. The software plots the five most frequent entities in the dataframe. This is basically for reporting; for a more refined analysis use the Data Explorer notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "printmd(\"**Analysis in progress...**\", color = \"red\")\n",
    "\n",
    "# Retrieve objects divided by year and organize them in a dic {key: [object]}\n",
    "list_global = []\n",
    "iterable = d.items()\n",
    "for key, value in tqdm(iterable):\n",
    "    year_list = []\n",
    "    listname = \"list_\" + key\n",
    "    year_list = value['authors'].tolist() # -> Configure here the column to target\n",
    "    year_list = [x.lower() for x in year_list if str(x) != \"nan\"] # -> All to lowercase to merge duplicates, e.g: \"Ethics\" and \"ethics\"\n",
    "    list_global.append(year_list)\n",
    "dic1 = dict(zip(yearlist, list_global)) # Structure: {df_year: [content, but divided by entry]}\n",
    "\n",
    "# Separate objects in the same string (e.g: authors are separated by commas)\n",
    "#typematch = re.compile(r\"(?<=')[\\w\\s]+(?=')\") -> not needed for authors, .split is enough\n",
    "dic2 = {} # structure: {df_year: [content, but merged]}\n",
    "for key, value in dic1.items():\n",
    "    dic2[key] = []\n",
    "    for x in value:\n",
    "        x = x.split(\", \")\n",
    "        for y in x:\n",
    "            dic2[key].append(y)\n",
    "\n",
    "# Create the dataframe\n",
    "df_authors = create_entity_dataframe(dic2, \"authors\")\n",
    "\n",
    "if df_all_status == \"empty\":\n",
    "    print(\"The authors dataframe is empty. So no normalization and no plots. Big bummer.\")\n",
    "    authors_file = exportdir + \"/data/Authors.csv\"\n",
    "    authors_file_norm = exportdir + \"/data/Authors_norm.csv\"\n",
    "    with open(authors_file, \"w\") as empty_csv:\n",
    "        pass\n",
    "    with open(authors_file_norm, \"w\") as empty_csv:\n",
    "        pass\n",
    "else:\n",
    "    # Create the normalized dataframe\n",
    "    df_authors_norm = create_normalized_dataframe(df_authors, \"authors\")\n",
    "\n",
    "    # Display the dataframe\n",
    "    clear_output()\n",
    "    message = (\"Trends computed. Displaying the trends of the 20 most frequent entities in the dataframe.\")\n",
    "    printmd(\"\\n\\n**\" + message + \"**\\n\\n\", color = \"green\")\n",
    "    printmd(\"**Trends**\", color = \"black\")\n",
    "    display(df_authors.head(20))\n",
    "\n",
    "    # Generate word cloud\n",
    "    generate_wordcloud(df_authors, \"authors\", \"total\", 50, \"white\")\n",
    "\n",
    "    # Get the list of the entities to plot (5 most common entities in the first dataframe)\n",
    "    authors_wordlist = []\n",
    "    i = 1\n",
    "    while i < 6:\n",
    "        word = df_authors.at[i, \"authors\"]\n",
    "        authors_wordlist.append(word)\n",
    "        i = i+1\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "    printmd(\"**Plots**\", color = \"black\")\n",
    "\n",
    "    # Plot the data\n",
    "    title = \"Authors \" + str(year0) + \" - \" + str(year1)\n",
    "    plot_df_top(df_authors, \"authors\", authors_wordlist, title)\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    # Plot the normalized data\n",
    "    # Important: plotting the same list as above!\n",
    "    title = \"Authors (normalized) \" + str(year0) + \" - \" + str(year1)\n",
    "    plot_df_top(df_authors_norm, \"authors\", authors_wordlist, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## *Caveat* : normalization and lemmas\n",
    "Normalization is performed in the same whay for each entity, i.e:  normalized entity = count of entity / number of papers. So a normalized value of 0.1 for, e.g. a keyword means that said entity is present in 10% of the entries in the subset. This is true for keywords, MeSH terms and authors, but not for anything which is based on lemmas. \n",
    "\n",
    "The reason is: any keyword, MeSH term or author can appear maximum once per entry (you don't list twice an author or a keyword). But lemmas can appear more than once per paper, so the normalized values of lemmas should not be considered as percentages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lemma trends in title/abstract\n",
    "Here we compute the trends for all the lemmas in the title/abstract fields. For this purpose we use all the text comprised in the columns \"title\", \"book_title\", \"abstract\", \"oabstract\". The lemmatization is performed with SpaCy. \n",
    "The trends are exported to .csv files. The software plots the five most frequent entities in the dataframe. This is basically for reporting; for a more refined analysis use the Data Explorer notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "d2 = d\n",
    "for key, item in d2.items():\n",
    "    item[\"text\"] = item.title.astype(str) + \" | \" + item.book_title.astype(str) + \" | \" + item.abstract.astype(str) + \" | \" + item.oabstract.astype(str)\n",
    "\n",
    "# Retrieve objects divided by year and organize them in a dic {key: [object]}\n",
    "list_global = []\n",
    "for key, value in d2.items():\n",
    "    year_list = []\n",
    "    listname = \"list_\" + key\n",
    "    year_list = value['text'].tolist() # -> Configure here the column to target\n",
    "    year_list = [x.lower() for x in year_list if str(x) != \"nan\"] # -> All to lowercase to merge duplicates, e.g: \"Ethics\" and \"ethics\"\n",
    "    list_global.append(year_list)\n",
    "dic1 = dict(zip(yearlist, list_global)) # Structure: {df_year: [content, but divided by entry]}\n",
    "\n",
    "message = \"This will take a while. Assuming 10 seconds per iteration, the proces will take about \" + str(len(dic1)*10/60) + \" minutes. Go grab yourself a coffee.\"\n",
    "printmd(\"\\n\\n**\" + message + \"**\\n\\n\", color = \"red\")\n",
    "\n",
    "dic2 = {} # structure: {df_year: [content, but processed by spacy]}\n",
    "iterable = dic1.items()\n",
    "for key, value in tqdm(iterable):\n",
    "    text = \" \".join(dic1[key])\n",
    "    file_doc = nlp(text, disable=[\"tagger\", \"parser\", \"attribute_ruler\", \"ner\"]) # disabling some components of the pipeline to speed up\n",
    "    complete_filtered_tokens = [preprocess_token(token) for token in file_doc if is_token_allowed(token)]\n",
    "    complete_filtered_tokens = [token for token in complete_filtered_tokens if token not in stoplist]\n",
    "    dic2[key] = complete_filtered_tokens\n",
    "\n",
    "# Create the dataframe\n",
    "df_lemmas = create_entity_dataframe(dic2, \"lemmas\")\n",
    "\n",
    "if df_all_status == \"empty\":\n",
    "    print(\"The dataframe is empty. So no normalization and no plots. Big bummer.\")\n",
    "    lemmas_file = exportdir + \"/data/Lemmas.csv\"\n",
    "    lemmas_file_norm = exportdir + \"/data/Lemmas_norm.csv\"\n",
    "    with open(keyword_file, \"w\") as empty_csv:\n",
    "        pass\n",
    "    with open(keyword_file_norm, \"w\") as empty_csv:\n",
    "        pass\n",
    "else:\n",
    "    # Create the normalized dataframe\n",
    "    df_lemmas_norm = create_normalized_dataframe(df_lemmas, \"lemmas\")\n",
    "\n",
    "    # Display the dataframe\n",
    "    clear_output()\n",
    "    message = (\"Trends computed. Displaying the trends of the 20 most frequent entities in the dataframe.\")\n",
    "    printmd(\"\\n\\n**\" + message + \"**\\n\\n\", color = \"green\")\n",
    "    printmd(\"**Trends**\", color = \"black\")\n",
    "    display(df_lemmas.head(20))\n",
    "\n",
    "    # Generate word cloud\n",
    "    generate_wordcloud(df_lemmas, \"lemmas\", \"total\", 50, \"white\")\n",
    "\n",
    "    # Get the list of the entities to plot (5 most common entities in the first dataframe)\n",
    "    lemmas_wordlist = []\n",
    "    i = 1\n",
    "    while i < 6:\n",
    "        word = df_lemmas.at[i, \"lemmas\"]\n",
    "        lemmas_wordlist.append(word)\n",
    "        i = i+1\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "    printmd(\"**Plots**\", color = \"black\")\n",
    "\n",
    "    # Plot the data\n",
    "    title = \"Lemmas in title and abstract \" + str(year0) + \" - \" + str(year1)\n",
    "    plot_df_top(df_lemmas, \"lemmas\", lemmas_wordlist, title)\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    # Plot the normalized data\n",
    "    # Important: plotting the same list as above!\n",
    "    title = \"Lemmas in title and abstract (normalized) \" + str(year0) + \" - \" + str(year1)\n",
    "    plot_df_top(df_lemmas_norm, \"lemmas\", lemmas_wordlist, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## COI statements and lemma trends in COI\n",
    "Here we compute the amount of papers that have a COI statement and the trends for all the lemmas in the COI statements. The lemmatization is performed with SpaCy. \n",
    "The trends are exported to .csv files. The software plots the presence of COI statements (raw and normalized) and the five most frequent entities in the dataframe. This is basically for reporting; for a more refined analysis use the Data Explorer notebook.\n",
    "\n",
    "It might be wise to specify an ad-hoc stoplist here, including e.g: \"declare\", \"interest\", \"compete\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Amount of COI statements\n",
    "coi_statements = len(df) - df['coi'].isna().sum()\n",
    "coi_percent = round(coi_statements/len(df)*100, 2)\n",
    "printmd(\"\\n\\n### COI statements - count\", color = \"black\")\n",
    "message = (\"Papers with a COI statement: \" + str(coi_statements) + \"/\" + str(len(df)) + \" (\" + str(coi_percent) + \"%)\")\n",
    "printmd(message, color = \"black\")\n",
    "\n",
    "coi_dict = {}\n",
    "for key, value in d.items():\n",
    "    papers_per_year = len(value)\n",
    "    papers_with_coi = len(value) - value['coi'].isna().sum()\n",
    "    coi_percent_per_year = round(papers_with_coi/papers_per_year*100, 2)\n",
    "    line = [papers_with_coi, coi_percent_per_year]\n",
    "    coi_dict[key] = line\n",
    "coi_df = pd.DataFrame.from_dict(coi_dict)\n",
    "coi_df = coi_df.rename(index={0: \"count\", 1: \"percent\"})\n",
    "coi_df = coi_df.fillna(0)\n",
    "indexes = [i for i in range(0,len(yearlist))]\n",
    "\n",
    "filename = \"Amount of COI statements\"\n",
    "coi_df.to_csv(exportdir + \"/data/\" + filename + \".csv\",  sep=';') # Specify the name of the file\n",
    "display(coi_df)\n",
    "plot_variations(coi_df, \"COI\", \"percent\")\n",
    "plot_variations(coi_df, \"COI\", \"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve objects divided by year and organize them in a dic {key: [object]}\n",
    "list_global = []\n",
    "for key, value in d.items():\n",
    "    year_list = []\n",
    "    listname = \"list_\" + key\n",
    "    year_list = value['coi'].tolist() # -> Configure here the column to target\n",
    "    year_list = [x.lower() for x in year_list if str(x) != \"nan\"] # -> All to lowercase to merge duplicates, e.g: \"Ethics\" and \"ethics\"\n",
    "    list_global.append(year_list)\n",
    "dic1 = dict(zip(yearlist, list_global)) # Structure: {df_year: [content, but divided by entry]}\n",
    "\n",
    "# Additional stoplist\n",
    "printmd(\"Here you can specify some additional stopwords to refine the analysis, excluding frequent tokens that might be relevant for general lemma frequency analysis, but not here, e.g: declare. Separate the words with a comma, or leave empty to keep using the standard stoplist. The list resets each time you run this block of code.\", color = \"black\")\n",
    "coi_additional_stoplist = input()\n",
    "coi_additional_stoplist = coi_additional_stoplist.split(\", \")\n",
    "coi_lemma_stoplist = stoplist\n",
    "for x in coi_additional_stoplist:\n",
    "    if x not in coi_lemma_stoplist:\n",
    "        coi_lemma_stoplist.append(x)\n",
    "\n",
    "message = \"This will take a while. Assuming 10 seconds per iteration, the proces will take about \" + str(len(dic1)*30/60) + \" minutes. Go grab yourself a coffee.\"\n",
    "printmd(\"\\n\\n**\" + message + \"**\\n\\n\", color = \"red\")\n",
    "\n",
    "dic2 = {} # structure: {df_year: [content, but processed by spacy]}\n",
    "iterable = dic1.items()\n",
    "for key, value in tqdm(iterable):\n",
    "    text = \" \".join(dic1[key])\n",
    "    file_doc = nlp(text, disable=[\"tagger\", \"parser\", \"attribute_ruler\", \"ner\"]) # disabling some components of the pipeline to speed up\n",
    "    complete_filtered_tokens = [preprocess_token(token) for token in file_doc if is_token_allowed(token)]\n",
    "    complete_filtered_tokens = [token for token in complete_filtered_tokens if token not in coi_lemma_stoplist]\n",
    "    dic2[key] = complete_filtered_tokens\n",
    "\n",
    "# Create the dataframe\n",
    "df_coi_lemmas = create_entity_dataframe(dic2, \"coi_lemmas\")\n",
    "\n",
    "if df_all_status == \"empty\":\n",
    "    print(\"The dataframe is empty. So no normalization and no plots. Big bummer.\")\n",
    "    coi_lemmas_file = exportdir + \"/data/Coi_lemmas.csv\"\n",
    "    coi_lemmas_file_norm = exportdir + \"/data/Coi_lemmas_norm.csv\"\n",
    "    with open(coi_lemmas_file, \"w\") as empty_csv:\n",
    "        pass\n",
    "    with open(coi_lemmas_file_norm, \"w\") as empty_csv:\n",
    "        pass\n",
    "else:\n",
    "    # Create the normalized dataframe\n",
    "    df_coi_lemmas_norm = create_normalized_dataframe(df_coi_lemmas, \"coi_lemmas\")\n",
    "\n",
    "    # Display the dataframe\n",
    "    clear_output()\n",
    "    message = (\"Trends computed. Displaying the trends of the 20 most frequent entities in the dataframe.\")\n",
    "    printmd(\"\\n\\n### COI statements - lemmas\", color = \"black\")\n",
    "    printmd(\"\\nStoplist:\\n\" + str(coi_lemma_stoplist), color = \"black\")\n",
    "    printmd(\"\\n\\n**\" + message + \"**\\n\\n\", color = \"green\")\n",
    "    printmd(\"**Trends**\", color = \"black\")\n",
    "    display(df_coi_lemmas.head(20))\n",
    "\n",
    "    # Generate word cloud\n",
    "    generate_wordcloud(df_coi_lemmas, \"coi_lemmas\", \"total\", 50, \"white\")\n",
    "\n",
    "    # Get the list of the entities to plot (5 most common entities in the first dataframe)\n",
    "    coi_lemmas_wordlist = []\n",
    "    i = 1\n",
    "    while i < 6:\n",
    "        word = df_coi_lemmas.at[i, \"coi_lemmas\"]\n",
    "        coi_lemmas_wordlist.append(word)\n",
    "        i = i+1\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "    printmd(\"**Plots**\", color = \"black\")\n",
    "\n",
    "    # Plot the data\n",
    "    title = \"Lemmas in COI \" + str(year0) + \" - \" + str(year1)\n",
    "    plot_df_top(df_coi_lemmas, \"coi_lemmas\", coi_lemmas_wordlist, title)\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    # Plot the normalized data\n",
    "    # Important: plotting the same list as above!\n",
    "    title = \"Lemmas in COI (normalized) \" + str(year0) + \" - \" + str(year1)\n",
    "    plot_df_top(df_coi_lemmas_norm, \"coi_lemmas\", coi_lemmas_wordlist, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Journal trends\n",
    "\n",
    "Here we compute the trends for all the journals in the dataset. The trends are exported to .csv files. The software plots the five most frequent entities in the dataframe. This is basically for reporting; for a more refined analysis use the Data Explorer notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "printmd(\"**Analysis in progress...**\", color = \"red\")\n",
    "\n",
    "# Retrieve objects divided by year and organize them in a dic {key: [object]}\n",
    "list_global = []\n",
    "iterable = d.items()\n",
    "for key, value in tqdm(iterable):\n",
    "    year_list = []\n",
    "    listname = \"list_\" + key\n",
    "    year_list = value['journal'].tolist() # -> Configure here the column to target\n",
    "    year_list = [x.lower() for x in year_list if str(x) != \"nan\"] # -> All to lowercase to merge duplicates, e.g: \"Ethics\" and \"ethics\"\n",
    "    list_global.append(year_list)\n",
    "dic1 = dict(zip(yearlist, list_global)) # Structure: {df_year: [content, but divided by entry]}\n",
    "\n",
    "# Create the dataframe\n",
    "df_journal = create_entity_dataframe(dic1, \"journal\")\n",
    "\n",
    "if df_all_status == \"empty\":\n",
    "    print(\"The journal dataframe is empty. So no normalization and no plots. Big bummer.\")\n",
    "    journal_file = exportdir + \"/data/Journal.csv\"\n",
    "    journal_file_norm = exportdir + \"/data/Journal_norm.csv\"\n",
    "    with open(journal_file, \"w\") as empty_csv:\n",
    "        pass\n",
    "    with open(journal_file_norm, \"w\") as empty_csv:\n",
    "        pass\n",
    "else:\n",
    "    # Create the normalized dataframe\n",
    "    df_journal_norm = create_normalized_dataframe(df_journal, \"journal\")\n",
    "\n",
    "    # Display the dataframe\n",
    "    clear_output()\n",
    "    message = (\"Trends computed. Displaying the trends of the 20 most frequent entities in the dataframe.\")\n",
    "    printmd(\"\\n\\n**\" + message + \"**\\n\\n\", color = \"green\")\n",
    "    printmd(\"**Trends**\", color = \"black\")\n",
    "    display(df_journal.head(20))\n",
    "\n",
    "    # Generate word cloud\n",
    "    generate_wordcloud(df_journal, \"journal\", \"total\", 50, \"white\")\n",
    "\n",
    "    # Get the list of the entities to plot (5 most common entities in the first dataframe)\n",
    "    journal_wordlist = []\n",
    "    i = 1\n",
    "    while i < 6:\n",
    "        word = df_journal.at[i, \"journal\"]\n",
    "        journal_wordlist.append(word)\n",
    "        i = i+1\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "    printmd(\"**Plots**\", color = \"black\")\n",
    "\n",
    "    # Plot the data\n",
    "    title = \"Journal \" + str(year0) + \" - \" + str(year1)\n",
    "    plot_df_top(df_journal, \"journal\", journal_wordlist, title)\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    # Plot the normalized data\n",
    "    # Important: plotting the same list as above!\n",
    "    title = \"Journal (normalized) \" + str(year0) + \" - \" + str(year1)\n",
    "    plot_df_top(df_journal_norm, \"journal\", journal_wordlist, title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
